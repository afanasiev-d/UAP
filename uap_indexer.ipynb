{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90251561-f4f1-4959-859b-769b59c99647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import base58\n",
    "import hashlib\n",
    "import requests\n",
    "import csv\n",
    "import os\n",
    "import warnings\n",
    "import gc\n",
    "import argparse\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "import time\n",
    "import csv\n",
    "import hashlib\n",
    "import os\n",
    "import psutil\n",
    "import csv\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import timedelta\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "\n",
    "# Set up logger\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)  # Set the logging level\n",
    "\n",
    "# Remove any default handlers associated with the logger\n",
    "logger.handlers = []\n",
    "\n",
    "# Create a file handler\n",
    "handler = logging.FileHandler('worklog.log', encoding='utf-8')\n",
    "# Create a rotating file handler\n",
    "handler.setLevel(logging.DEBUG)  # Set the handler level\n",
    "\n",
    "# Create a logging format\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "# Add the file handler to the logger\n",
    "logger.addHandler(handler)\n",
    "\n",
    "# Disable propagation to avoid printing to the console\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab17d7cb-dd91-41cc-b073-cf07abe2a33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "TRON_NODE_URL = \"https://trx-rpc.transatron.io\"\n",
    "\n",
    "def get_block_transactions(block_number):\n",
    "    url = f\"{TRON_NODE_URL}/wallet/getblockbynum\"\n",
    "    response = requests.post(url, json={\"num\": block_number})\n",
    "    response.raise_for_status()\n",
    "    block_data = response.json()\n",
    "    transactions = block_data.get(\"transactions\", [])\n",
    "    for tx in transactions:\n",
    "        tx[\"block_timestamp\"] = block_data[\"block_header\"][\"raw_data\"][\"timestamp\"]\n",
    "    return transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aa77b47-ad12-480d-91c2-34294a3b11da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache for addresses to speed up conversion\n",
    "address_cache = {}\n",
    "\n",
    "def hex_to_base58(hex_address):\n",
    "    \"\"\"\n",
    "    Converts a Tron address from hex format to Base58 format.\n",
    "    \"\"\"\n",
    "    if hex_address in address_cache:\n",
    "        return address_cache[hex_address]\n",
    "    if not hex_address.startswith(\"41\"):\n",
    "        raise ValueError(\"Invalid Tron address: Must start with '41'\")\n",
    "\n",
    "    address_bytes = bytes.fromhex(hex_address)\n",
    "    checksum = hashlib.sha256(hashlib.sha256(address_bytes).digest()).digest()[:4]\n",
    "    address_with_checksum = address_bytes + checksum\n",
    "    base58_address = base58.b58encode(address_with_checksum).decode()\n",
    "    address_cache[hex_address] = base58_address\n",
    "    return base58_address\n",
    "\n",
    "def save_to_csv(transactions, output_dir, block_number):\n",
    "    \"\"\"\n",
    "    Saves filtered transactions to CSV files organized by month.\n",
    "    Adds address conversion to TRON standard in the CSV.\n",
    "\n",
    "    Args:\n",
    "        transactions (list): List of transaction dictionaries.\n",
    "        output_dir (str): Directory to save the CSV files.\n",
    "        block_number (str): Block number for file name.\n",
    "    \"\"\"\n",
    "    files = {}\n",
    "    try:\n",
    "        for tx in transactions:\n",
    "            timestamp = tx.get(\"block_timestamp\")\n",
    "            if not timestamp:\n",
    "                continue  # Skip transactions without timestamp\n",
    "            month = datetime.datetime.fromtimestamp(timestamp / 1000).strftime(\"%Y-%m\")\n",
    "\n",
    "            # Prepare the output file and writer\n",
    "            if month not in files:\n",
    "                output_file = os.path.join(output_dir, f\"transactions-block-{block_number}.csv\")\n",
    "                file_exists = os.path.isfile(output_file)\n",
    "                csvfile = open(output_file, mode=\"a\", newline=\"\", encoding=\"utf-8\")\n",
    "\n",
    "                # Define the fieldnames (columns)\n",
    "                fieldnames = [\n",
    "                    \"txID\", \"signature\", \"contractRet\", \"ref_block_bytes\", \"ref_block_hash\", \"block_number\",\n",
    "                    \"timestamp\", \"owner_address_1\", \"owner_weight_1\", \"owner_address_2\", \"owner_weight_2\",\n",
    "                    \"actives_address_1\", \"actives_weight_1\", \"actives_address_2\", \"actives_weight_2\",\n",
    "                    \"owner_address\", \"owner_address_1_tron\", \"owner_address_2_tron\", \"actives_address_1_tron\", \"actives_address_2_tron\", \"block_timestamp\",\n",
    "                    \"delegate_reclaim_after_uap_count\", \"stake_count\", \"vote_count\", \"transfer_count\", \"is_transfer_to_the_second_owner\", \"additional_uap\"\n",
    "                ]\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                if not file_exists:\n",
    "                    writer.writeheader()\n",
    "                files[month] = (csvfile, writer)\n",
    "\n",
    "            # Extract the transaction data\n",
    "            txID = tx.get('txID', '')\n",
    "            signature = tx.get('signature', [''])[0]\n",
    "            contractRet = tx.get('ret', [{}])[0].get('contractRet', '')\n",
    "            ref_block_bytes = tx.get('raw_data', {}).get('ref_block_bytes', '')\n",
    "            ref_block_hash = tx.get('raw_data', {}).get('ref_block_hash', '')\n",
    "            block_timestamp = datetime.datetime.fromtimestamp(tx.get('block_timestamp', 0) / 1000).isoformat()\n",
    "\n",
    "            # Extract values from the contract\n",
    "            contract = tx.get('raw_data', {}).get('contract', [])\n",
    "            for item in contract:\n",
    "                parameter = item.get('parameter', {}).get('value', {})\n",
    "\n",
    "                # Extract 'owner' data\n",
    "                if 'owner' in parameter:\n",
    "                    owner_permission = parameter['owner']\n",
    "                    addresses = owner_permission.get('keys', [])\n",
    "\n",
    "                    # Handle missing second address by checking list length\n",
    "                    owner_address_1 = addresses[0].get('address', '')\n",
    "                    owner_weight_1 = addresses[0].get('weight', '')\n",
    "                    if len(addresses) > 1:\n",
    "                        owner_address_2 = addresses[1].get('address', '')\n",
    "                        owner_weight_2 = addresses[1].get('weight', '')\n",
    "                    else:\n",
    "                        owner_address_2 = ''\n",
    "                        owner_weight_2 = ''\n",
    "\n",
    "                    owner_address = parameter.get('owner_address', '')\n",
    "\n",
    "                    # Extract 'actives' data\n",
    "                    actives_address_1 = actives_weight_1 = actives_address_2 = actives_weight_2 = ''\n",
    "                    if 'actives' in parameter:\n",
    "                        actives = parameter['actives']\n",
    "                        actives_address_1 = actives[0]['keys'][0].get('address', '')\n",
    "                        actives_weight_1 = actives[0]['keys'][0].get('weight', '')\n",
    "                        if len(actives[0]['keys']) > 1:\n",
    "                            actives_address_2 = actives[0]['keys'][1].get('address', '')\n",
    "                            actives_weight_2 = actives[0]['keys'][1].get('weight', '')\n",
    "                        else:\n",
    "                            actives_address_2 = ''\n",
    "                            actives_weight_2 = ''\n",
    "\n",
    "                    # Convert addresses to TRON standard (Base58 format)\n",
    "                    owner_address_1_tron = hex_to_base58(owner_address_1)\n",
    "                    owner_address_2_tron = hex_to_base58(owner_address_2) if owner_address_2 else ''\n",
    "                    actives_address_1_tron = hex_to_base58(actives_address_1)\n",
    "                    actives_address_2_tron = hex_to_base58(actives_address_2) if actives_address_2 else ''\n",
    "\n",
    "                    # Write the extracted data to the CSV file\n",
    "                    writer = files[month][1]\n",
    "                    writer.writerow({\n",
    "                        \"txID\": txID,\n",
    "                        \"signature\": signature,\n",
    "                        \"contractRet\": contractRet,\n",
    "                        \"ref_block_bytes\": ref_block_bytes,\n",
    "                        \"ref_block_hash\": ref_block_hash,\n",
    "                        \"block_number\": block_number,\n",
    "                        \"timestamp\": datetime.datetime.fromtimestamp(timestamp / 1000).isoformat(),\n",
    "                        \"owner_address_1\": owner_address_1,\n",
    "                        \"owner_weight_1\": owner_weight_1,\n",
    "                        \"owner_address_2\": owner_address_2,\n",
    "                        \"owner_weight_2\": owner_weight_2,\n",
    "                        \"actives_address_1\": actives_address_1,\n",
    "                        \"actives_weight_1\": actives_weight_1,\n",
    "                        \"actives_address_2\": actives_address_2,\n",
    "                        \"actives_weight_2\": actives_weight_2,\n",
    "                        \"owner_address\": owner_address,\n",
    "                        \"owner_address_1_tron\": owner_address_1_tron,\n",
    "                        \"owner_address_2_tron\": owner_address_2_tron,\n",
    "                        \"actives_address_1_tron\": actives_address_1_tron,\n",
    "                        \"actives_address_2_tron\": actives_address_2_tron,\n",
    "                        \"block_timestamp\": block_timestamp,\n",
    "                        \"delegate_reclaim_after_uap_count\": 0,  # Assuming constant field\n",
    "                        \"stake_count\": 0,  # Assuming constant field\n",
    "                        \"vote_count\": 0,  # Assuming constant field\n",
    "                        \"transfer_count\": 0,  # Assuming constant field\n",
    "                        \"is_transfer_to_the_second_owner\": False,  # Assuming constant field\n",
    "                        \"additional_uap\": 0  # Assuming constant field\n",
    "                    })\n",
    "\n",
    "    finally:\n",
    "        # Close all open files\n",
    "        for csvfile, _ in files.values():\n",
    "            csvfile.close()\n",
    "\n",
    "    #print(\"CSV file has been written successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b620cf9-3a7d-4bbd-acf0-2e3c10869dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_and_save_to_csv(output_dir, existing_csv_name, block_number):\n",
    "    \"\"\"\n",
    "    Concatenates the new block's CSV data to the existing general CSV (UAP transactions).\n",
    "    The new block's data is identified by the block_number and saved to the same output file.\n",
    "\n",
    "    Args:\n",
    "        output_dir (str): Directory where the CSV files are located.\n",
    "        existing_csv_name (str): Filename of the existing general CSV (UAP transactions).\n",
    "        block_number (str): The current block number to identify the CSV file for the new block.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the concatenated data.\n",
    "    \"\"\"\n",
    "    # Construct the paths for the existing CSV and the new CSV based on block_number\n",
    "    existing_csv = os.path.join(output_dir, existing_csv_name)\n",
    "    new_csv = os.path.join(output_dir, f\"transactions-block-{block_number}.csv\")\n",
    "\n",
    "    # Load the existing general CSV\n",
    "    if os.path.exists(existing_csv):\n",
    "        existing_df = pd.read_csv(existing_csv)\n",
    "    else:\n",
    "        # If the general CSV doesn't exist, initialize as empty DataFrame\n",
    "        #print(f\"Warning: {existing_csv} not found. Will create a new file with the current data.\")\n",
    "        existing_df = pd.DataFrame()\n",
    "\n",
    "    # Load the new block's CSV data (based on block_number)\n",
    "    if os.path.exists(new_csv):\n",
    "        new_df = pd.read_csv(new_csv)\n",
    "    else:\n",
    "        #rint(f\"Warning: {new_csv} not found. Returning the existing data.\")\n",
    "        new_df = pd.DataFrame()\n",
    "\n",
    "    # If existing_df is empty, just save the new data\n",
    "    if existing_df.empty:\n",
    "        combined_df = new_df\n",
    "    else:\n",
    "        # Concatenate the existing and new DataFrames\n",
    "        combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "\n",
    "        # Remove duplicates based on 'owner_address', keeping the last one\n",
    "        #combined_df = combined_df.drop_duplicates(subset='owner_address', keep='first')\n",
    "\n",
    "        # Optionally, here you can add logic to modify or update rows for the same `owner_address`\n",
    "        # For example, here you can apply updates like changing `stake_count`, etc.\n",
    "\n",
    "        # Example update: Let's add 1 to `stake_count` for all rows with the same `owner_address`\n",
    "        #combined_df['stake_count'] = combined_df.groupby('owner_address')['stake_count'].transform(lambda x: x + 1)\n",
    "\n",
    "    # Save the updated DataFrame back to the existing general CSV\n",
    "    combined_df.to_csv(existing_csv, index=False)\n",
    "\n",
    "    #print(f\"Combined data saved to {existing_csv}\")\n",
    "\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32c2b3f5-f318-4330-ad3f-1edf0dd595b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_block_csv(output_dir, block_number):\n",
    "    \"\"\"\n",
    "    Deletes the CSV file generated for a particular block from the directory.\n",
    "\n",
    "    Args:\n",
    "        output_dir (str): Directory where the CSV files are located.\n",
    "        block_number (str): Block number to identify the CSV file to delete.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Construct the path for the block CSV file based on block_number\n",
    "    block_csv = os.path.join(output_dir, f\"transactions-block-{block_number}.csv\")\n",
    "\n",
    "    # Check if the file exists and delete it\n",
    "    if os.path.exists(block_csv):\n",
    "        os.remove(block_csv)\n",
    "        #print(f\"Deleted {block_csv} successfully.\")\n",
    "    #else:\n",
    "        #print(f\"File {block_csv} does not exist. No action taken.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86c540d8-fb37-4ed4-aa5b-d10662ad2788",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_number=68323882\n",
    "\n",
    "agg_file_dir='UAP_agg.csv'\n",
    "block_last=block_number\n",
    "\n",
    "get_block_temp=get_block_transactions(block_number)\n",
    "save_to_csv(get_block_temp, '', block_number)\n",
    "#concat_and_save_to_csv('', agg_file_dir, block_number)\n",
    "#update_additional_uap_from_transactions(get_block_temp, agg_file_dir)\n",
    "#update_delegate_reclaim_count_from_transactions(get_block_temp, agg_file_dir)\n",
    "#update_stake_count_from_transactions(get_block_temp, agg_file_dir)\n",
    "#update_uap_agg_from_transfers(get_block_temp, agg_file_dir)\n",
    "#filter_invalid_rows_and_save(agg_file_dir)\n",
    "#delete_block_csv('', block_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69781890-f55a-4365-bc63-e68e2d4b83f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_block(block_number):\n",
    "    \"\"\"\n",
    "    Process a single block:\n",
    "    - Get block transactions\n",
    "    - Save to CSV\n",
    "    - Concatenate with existing data\n",
    "    - Delete the temporary block CSV file\n",
    "    \"\"\"\n",
    "    get_block_temp = get_block_transactions(block_number)\n",
    "    save_to_csv(get_block_temp, '', block_number)\n",
    "    #concat_and_save_to_csv('', agg_file_dir, block_number)\n",
    "    #delete_block_csv('', block_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a901202-7806-419d-a144-8f17061aff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_and_cleanup_csvs(output_filename=\"aggregated_transactions.csv\"):\n",
    "    # Get a list of all files in the current directory starting with \"transaction\"\n",
    "    transaction_files = [file for file in os.listdir() if file.startswith(\"transaction\") and file.endswith(\".csv\")]\n",
    "    \n",
    "    if not transaction_files:\n",
    "        print(\"No transaction files found.\")\n",
    "        return\n",
    "\n",
    "    # List to hold dataframes\n",
    "    dataframes = []\n",
    "\n",
    "    # Read each transaction file into a DataFrame and append to the list\n",
    "    for file in transaction_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "    # Concatenate all dataframes into one\n",
    "    if dataframes:\n",
    "        aggregated_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "        # Save the aggregated dataframe to a CSV\n",
    "        aggregated_df.to_csv(output_filename, index=False)\n",
    "        print(f\"Aggregated file saved as {output_filename}\")\n",
    "\n",
    "        # Delete the individual transaction files\n",
    "        for file in transaction_files:\n",
    "            try:\n",
    "                os.remove(file)\n",
    "                print(f\"Deleted {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting {file}: {e}\")\n",
    "    else:\n",
    "        print(\"No dataframes to aggregate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "357126a6-195f-4da8-a8ba-24685121a0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_and_cleanup_csvs(output_filename=\"aggregated_transactions.csv\"):\n",
    "    \"\"\"\n",
    "    Aggregates all CSV files in the current directory that start with 'transaction'\n",
    "    into one CSV file and deletes the individual files.\n",
    "\n",
    "    Args:\n",
    "        output_filename (str): The name of the output file for the aggregated data.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with keys 'aggregated', 'deleted', and 'errors', containing:\n",
    "              - 'aggregated': True if the aggregation was successful, False otherwise.\n",
    "              - 'deleted': A list of files successfully deleted.\n",
    "              - 'errors': A list of errors encountered during the process.\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        \"aggregated\": False,\n",
    "        \"deleted\": [],\n",
    "        \"errors\": []\n",
    "    }\n",
    "\n",
    "    # Get a list of all files in the current directory starting with \"transaction\"\n",
    "    transaction_files = [file for file in os.listdir() if file.startswith(\"transaction\") and file.endswith(\".csv\")]\n",
    "\n",
    "    if not transaction_files:\n",
    "        return result\n",
    "\n",
    "    # List to hold dataframes\n",
    "    dataframes = []\n",
    "\n",
    "    # Read each transaction file into a DataFrame and append to the list\n",
    "    for file in transaction_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            result[\"errors\"].append({\"file\": file, \"error\": str(e)})\n",
    "\n",
    "    # Concatenate all dataframes into one\n",
    "    if dataframes:\n",
    "        aggregated_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "        try:\n",
    "            # Save the aggregated dataframe to a CSV\n",
    "            aggregated_df.to_csv(output_filename, index=False)\n",
    "            result[\"aggregated\"] = True\n",
    "        except Exception as e:\n",
    "            result[\"errors\"].append({\"file\": output_filename, \"error\": str(e)})\n",
    "            return result\n",
    "\n",
    "        # Delete the individual transaction files\n",
    "        for file in transaction_files:\n",
    "            try:\n",
    "                os.remove(file)\n",
    "                result[\"deleted\"].append(file)\n",
    "            except Exception as e:\n",
    "                result[\"errors\"].append({\"file\": file, \"error\": str(e)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0e6cae4-4990-4acd-9d21-4ad9fe6435b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(block_last, batch_size):\n",
    "    \n",
    "        # Process blocks in parallel using ThreadPoolExecutor\n",
    "        block_last = block_last  # Adjust to your starting block\n",
    "        block_numbers = range(block_last, block_last + batch_size)  # Adjust the range of blocks you want to process\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_block = {executor.submit(process_block, block_number): block_number for block_number in block_numbers}\n",
    "\n",
    "            # Track progress with tqdm for parallel tasks\n",
    "            for future in tqdm(as_completed(future_to_block), total=len(block_numbers), desc=\"Processing blocks\", unit=\"block\"):\n",
    "                block_number = future_to_block[future]\n",
    "                try:\n",
    "                    future.result()  # Wait for the block processing to finish\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "                    #print(f\"Block {block_number} generated an error: {e}\")\n",
    "                    \n",
    "        aggregate_and_cleanup_csvs(f\"aggregated_transactions-{block_last}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91c72f0e-76d1-4d3e-a53c-d606f3ef655c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing blocks: 100%|██████████| 10000/10000 [02:07<00:00, 78.72block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:32<00:00, 65.78block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:38<00:00, 101.77block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:09<00:00, 77.48block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:19<00:00, 71.58block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:36<00:00, 104.03block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:19<00:00, 71.93block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:14<00:00, 74.56block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:36<00:00, 103.93block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:24<00:00, 69.37block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:09<00:00, 77.16block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:34<00:00, 105.65block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:12<00:00, 75.29block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:51<00:00, 89.39block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:38<00:00, 101.40block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:11<00:00, 75.93block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:44<00:00, 96.09block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:53<00:00, 88.21block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:35<00:00, 64.26block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:48<00:00, 92.37block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:07<00:00, 78.54block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:45<00:00, 60.57block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:38<00:00, 101.13block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:13<00:00, 74.97block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:35<00:00, 64.36block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:38<00:00, 101.72block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:21<00:00, 70.66block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:29<00:00, 67.01block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:37<00:00, 102.48block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:21<00:00, 70.72block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:17<00:00, 72.49block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:34<00:00, 105.56block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:09<00:00, 76.93block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:54<00:00, 87.36block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:33<00:00, 107.04block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:05<00:00, 79.37block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:52<00:00, 88.92block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:49<00:00, 91.27block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:47<00:00, 59.65block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:55<00:00, 86.58block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:56<00:00, 85.47block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:54<00:00, 57.38block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:49<00:00, 90.97block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:07<00:00, 78.54block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:42<00:00, 61.36block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:41<00:00, 98.18block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:08<00:00, 77.82block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:31<00:00, 66.09block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:38<00:00, 101.44block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:15<00:00, 73.73block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:33<00:00, 65.33block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:36<00:00, 104.16block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:08<00:00, 77.69block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:08<00:00, 77.90block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:30<00:00, 109.91block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:06<00:00, 79.28block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:02<00:00, 81.96block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:38<00:00, 101.14block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:40<00:00, 62.47block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:06<00:00, 78.95block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:42<00:00, 97.31block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:30<00:00, 66.29block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:52<00:00, 88.99block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:51<00:00, 90.05block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:30<00:00, 66.56block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:55<00:00, 86.47block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:31<00:00, 66.01block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [03:25<00:00, 48.70block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:34<00:00, 64.64block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:45<00:00, 60.45block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [03:03<00:00, 54.43block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:43<00:00, 96.47block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [03:17<00:00, 50.70block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:45<00:00, 60.27block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:35<00:00, 105.24block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:05<00:00, 79.78block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:10<00:00, 76.41block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:38<00:00, 101.06block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:35<00:00, 64.44block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:23<00:00, 69.65block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:43<00:00, 96.71block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:41<00:00, 61.94block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:16<00:00, 73.51block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:44<00:00, 95.36block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:42<00:00, 61.42block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:05<00:00, 79.92block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:50<00:00, 90.26block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:47<00:00, 59.62block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:53<00:00, 88.00block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:00<00:00, 82.97block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:43<00:00, 61.08block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:42<00:00, 97.31block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:56<00:00, 85.48block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:20<00:00, 71.42block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:32<00:00, 107.55block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:57<00:00, 85.23block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:12<00:00, 75.37block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:33<00:00, 106.59block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:25<00:00, 68.94block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:27<00:00, 67.59block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:39<00:00, 100.87block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:30<00:00, 66.28block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:22<00:00, 70.15block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:45<00:00, 94.97block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:36<00:00, 64.05block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:13<00:00, 75.01block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:47<00:00, 93.01block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:44<00:00, 60.74block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:08<00:00, 77.93block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:48<00:00, 92.49block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:44<00:00, 60.79block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:54<00:00, 87.13block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:48<00:00, 92.21block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:21<00:00, 70.50block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:41<00:00, 98.12block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:58<00:00, 84.54block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:26<00:00, 68.38block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:44<00:00, 95.84block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:22<00:00, 70.37block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:53<00:00, 57.55block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:45<00:00, 94.44block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:36<00:00, 63.92block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:43<00:00, 61.14block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:44<00:00, 95.38block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:29<00:00, 66.75block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:36<00:00, 63.87block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:46<00:00, 94.20block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:42<00:00, 61.37block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:21<00:00, 70.60block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:46<00:00, 93.93block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:51<00:00, 58.32block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:15<00:00, 73.84block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:45<00:00, 94.73block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:37<00:00, 63.32block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:53<00:00, 87.93block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:48<00:00, 92.39block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:19<00:00, 71.75block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:45<00:00, 95.23block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:09<00:00, 77.36block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:56<00:00, 56.67block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:44<00:00, 95.63block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:14<00:00, 74.21block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:53<00:00, 57.76block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:44<00:00, 96.08block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:21<00:00, 70.60block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:37<00:00, 63.67block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:44<00:00, 95.40block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:31<00:00, 66.21block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:29<00:00, 66.71block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:45<00:00, 95.15block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:31<00:00, 65.86block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:25<00:00, 68.93block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:39<00:00, 100.00block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:28<00:00, 67.36block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:05<00:00, 79.83block/s] \n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:40<00:00, 99.78block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:19<00:00, 71.80block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:50<00:00, 90.18block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:53<00:00, 88.30block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [03:00<00:00, 55.47block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:54<00:00, 87.17block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:03<00:00, 80.80block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:58<00:00, 56.12block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:45<00:00, 94.35block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:10<00:00, 76.62block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:40<00:00, 62.36block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:42<00:00, 97.71block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:15<00:00, 73.74block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:34<00:00, 64.79block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:38<00:00, 101.71block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:25<00:00, 68.72block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:38<00:00, 63.10block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:40<00:00, 99.95block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:24<00:00, 69.25block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:19<00:00, 71.94block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:39<00:00, 100.35block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:18<00:00, 72.38block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:04<00:00, 80.06block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:45<00:00, 94.64block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [03:01<00:00, 55.11block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:14<00:00, 74.37block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:56<00:00, 85.52block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [03:04<00:00, 54.21block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:09<00:00, 76.94block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:07<00:00, 78.65block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:52<00:00, 57.94block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:49<00:00, 90.95block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:11<00:00, 75.99block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:43<00:00, 61.22block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:43<00:00, 96.44block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:11<00:00, 75.87block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:34<00:00, 64.80block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:38<00:00, 101.40block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:02<00:00, 81.41block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:06<00:00, 78.83block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:33<00:00, 107.06block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:58<00:00, 84.48block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:00<00:00, 82.80block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:40<00:00, 99.83block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:34<00:00, 64.83block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:15<00:00, 73.88block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:46<00:00, 93.51block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:38<00:00, 63.15block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:08<00:00, 77.55block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:44<00:00, 95.68block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:35<00:00, 64.31block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:53<00:00, 87.83block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:49<00:00, 91.30block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:31<00:00, 65.81block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:44<00:00, 95.87block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:55<00:00, 86.48block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:25<00:00, 68.74block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:36<00:00, 103.67block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:48<00:00, 91.77block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:09<00:00, 77.18block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:32<00:00, 108.33block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:52<00:00, 88.87block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:06<00:00, 79.01block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:36<00:00, 103.32block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:19<00:00, 71.62block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:25<00:00, 68.91block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:38<00:00, 101.69block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:17<00:00, 72.65block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:12<00:00, 75.52block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:38<00:00, 101.42block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:22<00:00, 70.02block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:04<00:00, 80.16block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:44<00:00, 95.26block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:23<00:00, 69.74block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:55<00:00, 86.89block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:15<00:00, 73.53block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:39<00:00, 62.83block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:43<00:00, 96.66block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:45<00:00, 94.93block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:10<00:00, 76.86block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:38<00:00, 101.69block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:45<00:00, 94.80block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:04<00:00, 80.32block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:33<00:00, 107.27block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:04<00:00, 80.15block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:19<00:00, 71.65block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:35<00:00, 104.99block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:11<00:00, 75.94block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:10<00:00, 76.72block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:32<00:00, 108.01block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:05<00:00, 79.84block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:57<00:00, 84.77block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:32<00:00, 108.33block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:14<00:00, 74.61block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:57<00:00, 84.97block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:38<00:00, 101.13block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:19<00:00, 71.78block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:51<00:00, 90.00block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:38<00:00, 101.32block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:07<00:00, 78.67block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:38<00:00, 101.91block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:51<00:00, 90.07block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:16<00:00, 73.50block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:17<00:00, 72.53block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:54<00:00, 87.35block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:21<00:00, 70.68block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:35<00:00, 104.54block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:08<00:00, 77.69block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:10<00:00, 76.35block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:29<00:00, 111.42block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:52<00:00, 89.15block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:55<00:00, 86.30block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:33<00:00, 107.22block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [03:07<00:00, 53.37block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:02<00:00, 81.69block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:35<00:00, 104.68block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:07<00:00, 78.27block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:02<00:00, 81.32block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:37<00:00, 102.69block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:05<00:00, 79.74block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:43<00:00, 96.37block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:41<00:00, 98.81block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:00<00:00, 83.16block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:38<00:00, 101.99block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:52<00:00, 88.88block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:33<00:00, 65.31block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:40<00:00, 99.43block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:58<00:00, 84.54block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:21<00:00, 70.83block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:37<00:00, 102.98block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:59<00:00, 83.46block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:16<00:00, 73.38block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:39<00:00, 100.85block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:02<00:00, 81.50block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:10<00:00, 76.83block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:39<00:00, 100.20block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:06<00:00, 79.22block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:08<00:00, 77.79block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:35<00:00, 104.48block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:54<00:00, 87.55block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:47<00:00, 92.99block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:33<00:00, 107.24block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:55<00:00, 86.58block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:42<00:00, 97.26block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:43<00:00, 96.22block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:22<00:00, 70.05block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:44<00:00, 96.06block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:46<00:00, 94.22block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:11<00:00, 75.97block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:37<00:00, 102.51block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:55<00:00, 86.43block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:26<00:00, 68.40block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:41<00:00, 98.58block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:58<00:00, 84.65block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:18<00:00, 72.00block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:38<00:00, 101.34block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:03<00:00, 80.72block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:13<00:00, 75.11block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:33<00:00, 106.72block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:02<00:00, 81.54block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:05<00:00, 79.82block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:34<00:00, 105.92block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:09<00:00, 77.30block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:59<00:00, 83.80block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:47<00:00, 93.32block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:37<00:00, 63.54block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:14<00:00, 74.34block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:42<00:00, 97.65block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:22<00:00, 70.20block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:49<00:00, 91.48block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:43<00:00, 97.05block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:13<00:00, 74.71block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:43<00:00, 96.79block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:48<00:00, 92.15block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:07<00:00, 78.42block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:37<00:00, 103.01block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:53<00:00, 88.10block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:06<00:00, 78.83block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:32<00:00, 108.49block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:48<00:00, 92.09block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:58<00:00, 84.39block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:29<00:00, 111.52block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:47<00:00, 92.87block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:48<00:00, 91.85block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:31<00:00, 109.86block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:08<00:00, 77.61block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:57<00:00, 85.26block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:31<00:00, 109.76block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [02:06<00:00, 79.15block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:45<00:00, 95.10block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:28<00:00, 112.41block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [01:40<00:00, 99.13block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [00:57<00:00, 173.18block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [00:57<00:00, 174.20block/s]\n",
      "Processing blocks: 100%|██████████| 10000/10000 [00:57<00:00, 174.49block/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10000\n",
    "block_last = 65700000\n",
    "while block_last < 69200000:\n",
    "\n",
    "    process_batch(block_last, batch_size)\n",
    "    block_last =  block_last + batch_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b05b683a-52bd-43bf-9a23-1b60b5f618ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Конфігурація для багатопотокової обробки\n",
    "batch_size = 100  # наприклад, розмір пачки\n",
    "offset = 0  # початковий зсув\n",
    "max_workers = 3  # кількість потоків"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4da9a347-f2c7-4ad1-8b1f-2da4b82de131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal max_workers: 24\n"
     ]
    }
   ],
   "source": [
    "# Get the number of logical CPU cores\n",
    "num_cores = os.cpu_count()\n",
    "\n",
    "# Get available memory in MB\n",
    "available_memory = psutil.virtual_memory().available / (1024 * 1024)\n",
    "\n",
    "# Estimate the memory usage per thread (in MB)\n",
    "memory_per_thread = 100  # Adjust based on your task's requirements\n",
    "# Calculate max workers based on memory\n",
    "max_workers_memory = int(available_memory // memory_per_thread)\n",
    "\n",
    "# Choose the smaller value between CPU-bound and memory-bound limits\n",
    "max_workers = min(num_cores * 2, max_workers_memory)  # Assuming I/O-bound tasks\n",
    "print(f\"Optimal max_workers: {max_workers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "572dc820-7ae7-471e-977d-cc2fc470cd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing blocks: 100%|██████████| 1000/1000 [00:15<00:00, 63.11block/s]\n"
     ]
    }
   ],
   "source": [
    "# Process blocks in parallel using ThreadPoolExecutor\n",
    "block_last = 66000000  # Adjust to your starting block\n",
    "block_numbers = range(block_last, block_last + 1000)  # Adjust the range of blocks you want to process\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    future_to_block = {executor.submit(process_block, block_number): block_number for block_number in block_numbers}\n",
    "\n",
    "    # Track progress with tqdm for parallel tasks\n",
    "    for future in tqdm(as_completed(future_to_block), total=len(block_numbers), desc=\"Processing blocks\", unit=\"block\"):\n",
    "        block_number = future_to_block[future]\n",
    "        try:\n",
    "            future.result()  # Wait for the block processing to finish\n",
    "        except Exception as e:\n",
    "            continue\n",
    "            #print(f\"Block {block_number} generated an error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94efb3d0-57d9-4e19-b681-8930ff2f72e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_and_cleanup_csvs(output_filename=\"aggregated_transactions.csv\"):\n",
    "    # Get a list of all files in the current directory starting with \"transaction\"\n",
    "    transaction_files = [file for file in os.listdir() if file.startswith(\"transaction\") and file.endswith(\".csv\")]\n",
    "    \n",
    "    if not transaction_files:\n",
    "        print(\"No transaction files found.\")\n",
    "        return\n",
    "\n",
    "    # List to hold dataframes\n",
    "    dataframes = []\n",
    "\n",
    "    # Read each transaction file into a DataFrame and append to the list\n",
    "    for file in transaction_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "    # Concatenate all dataframes into one\n",
    "    if dataframes:\n",
    "        aggregated_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "        # Save the aggregated dataframe to a CSV\n",
    "        aggregated_df.to_csv(output_filename, index=False)\n",
    "        print(f\"Aggregated file saved as {output_filename}\")\n",
    "\n",
    "        # Delete the individual transaction files\n",
    "        for file in transaction_files:\n",
    "            try:\n",
    "                os.remove(file)\n",
    "                print(f\"Deleted {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting {file}: {e}\")\n",
    "    else:\n",
    "        print(\"No dataframes to aggregate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62a07127-052f-42d0-849c-46dae59f4548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_files_with_prefix(prefix, extension=\"csv\"):\n",
    "    \"\"\"\n",
    "    Deletes all files in the current directory starting with the specified prefix \n",
    "    and having the specified extension.\n",
    "    \n",
    "    Args:\n",
    "        prefix (str): The prefix to look for in file names.\n",
    "        extension (str): The file extension to match (default is \"csv\").\n",
    "    \"\"\"\n",
    "    # Get a list of all files matching the prefix and extension\n",
    "    files_to_delete = [file for file in os.listdir() if file.startswith(prefix) and file.endswith(f\".{extension}\")]\n",
    "    \n",
    "    if not files_to_delete:\n",
    "        #print(f\"No files found with prefix '{prefix}' and extension '{extension}'.\")\n",
    "        return\n",
    "\n",
    "    # Delete the matching files\n",
    "    for file in files_to_delete:\n",
    "        try:\n",
    "            os.remove(file)\n",
    "            #print(f\"Deleted file: {file}\")\n",
    "        except Exception as e:\n",
    "                continue\n",
    "            #print(f\"Error deleting {file}: {e}\")\n",
    "\n",
    "# Example usage\n",
    "delete_files_with_prefix(\"transaction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a64244de-5562-4971-9211-074651b97819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_and_cleanup_csvs_1(output_filename=\"aggregated_transactions.csv\", prefix=\"transaction\"):\n",
    "    \"\"\"\n",
    "    Aggregates all CSV files in the current directory that start with 'transaction'\n",
    "    into one CSV file and deletes the individual files.\n",
    "\n",
    "    Args:\n",
    "        output_filename (str): The name of the output file for the aggregated data.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with keys 'aggregated', 'deleted', and 'errors', containing:\n",
    "              - 'aggregated': True if the aggregation was successful, False otherwise.\n",
    "              - 'deleted': A list of files successfully deleted.\n",
    "              - 'errors': A list of errors encountered during the process.\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        \"aggregated\": False,\n",
    "        \"deleted\": [],\n",
    "        \"errors\": []\n",
    "    }\n",
    "\n",
    "    # Get a list of all files in the current directory starting with \"transaction\"\n",
    "    transaction_files = [file for file in os.listdir() if file.startswith(prefix) and file.endswith(\".csv\")]\n",
    "\n",
    "    if not transaction_files:\n",
    "        return result\n",
    "\n",
    "    # List to hold dataframes\n",
    "    dataframes = []\n",
    "\n",
    "    # Read each transaction file into a DataFrame and append to the list\n",
    "    for file in transaction_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            result[\"errors\"].append({\"file\": file, \"error\": str(e)})\n",
    "\n",
    "    # Concatenate all dataframes into one\n",
    "    if dataframes:\n",
    "        aggregated_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "        try:\n",
    "            # Save the aggregated dataframe to a CSV\n",
    "            aggregated_df.to_csv(output_filename, index=False)\n",
    "            result[\"aggregated\"] = True\n",
    "        except Exception as e:\n",
    "            result[\"errors\"].append({\"file\": output_filename, \"error\": str(e)})\n",
    "            return result\n",
    "\n",
    "        # Delete the individual transaction files\n",
    "        #for file in transaction_files:\n",
    "        #    try:\n",
    "        #        os.remove(file)\n",
    "        #        result[\"deleted\"].append(file)\n",
    "        #    except Exception as e:\n",
    "        #        result[\"errors\"].append({\"file\": file, \"error\": str(e)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "142f3370-d2d0-443d-ad1b-f31a0c19daab",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_and_cleanup_csvs_1(output_filename=\"uap_agg.csv\", prefix=\"aggregated_transactions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
